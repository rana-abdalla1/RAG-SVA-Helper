{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ipk_WED8mnRL",
      "metadata": {
        "id": "Ipk_WED8mnRL"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain==0.3.0 langchain-community==0.3.0 langchain-openai==0.2.0 \\\n",
        "                chromadb openai tiktoken\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"dummy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "LZRo49kGmu_G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZRo49kGmu_G",
        "outputId": "f9301f25-f12c-4526-e498-04176ac01ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 20000 docs (truncated).\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "VERT_JSONL_PATH = \"/content/drive/MyDrive/VERT.json\"\n",
        "\n",
        "def load_vert_rows(path):\n",
        "    rows=[]\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                try: rows.append(json.loads(line))\n",
        "                except: pass\n",
        "    return rows\n",
        "\n",
        "rows = load_vert_rows(VERT_JSONL_PATH)\n",
        "\n",
        "def rows_to_docs(rows, max_chars=1200):\n",
        "    docs=[]\n",
        "    for r in rows:\n",
        "        code = (r.get(\"Code\") or \"\").strip()\n",
        "        text = f\"CODE:\\n{code[:max_chars]}\"\n",
        "        docs.append(Document(page_content=text, metadata={}))\n",
        "    return docs\n",
        "\n",
        "docs = rows_to_docs(rows)\n",
        "print(f\"Prepared {len(docs)} docs (truncated).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "btYz_W21m4R-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btYz_W21m4R-",
        "outputId": "59bda83b-7a80-49ee-b8c6-162c7cf0a2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks after split: 20000\n",
            "Vector store ready.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "docs_split = splitter.split_documents(docs)\n",
        "\n",
        "vstore = Chroma(collection_name=\"vert_sva_gen\", embedding_function=emb)\n",
        "\n",
        "BATCH = 128\n",
        "for i in range(0, len(docs_split), BATCH):\n",
        "    vstore.add_documents(docs_split[i:i+BATCH])\n",
        "\n",
        "\n",
        "retriever = vstore.as_retriever(search_kwargs={\"k\": 2})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UxwaHxnRm9h3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxwaHxnRm9h3",
        "outputId": "8bf622d8-7710-42ef-c753-2216a081f59e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: WARNING! response_format is not default parameter.\n",
            "                response_format was transferred to model_kwargs.\n",
            "                Please confirm that response_format is what you intended.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import json as pyjson\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    response_format={\"type\": \"json_object\"},\n",
        ")\n",
        "\n",
        "SYS = \"\"\"You generate strict SystemVerilog Assertions from a requirement and signals.\n",
        "Return only JSON with fields: assertion, anti_vacuity_cover, notes.\n",
        "\n",
        "HARD RULES:\n",
        "- assertion: include a NAMED property + `assert property (NAME);`\n",
        "- anti_vacuity_cover: must be a `cover property (...)` (never assert)\n",
        "- Both properties: `@(posedge {clock}) disable iff (!{reset})`\n",
        "- For (valid && !ready) backpressure, use overlapped `|->` so stability starts same cycle.\n",
        "- Use $stable(signal) / $past(signal). NEVER write `data == $stable(data)`.\n",
        "- Use `until` / `until_with` to hold until release; use `until_with` to include the release cycle.\n",
        "- Cover must witness: antecedent occurs, then release later (e.g. `(valid && !ready) ##[1:$] ready`).\n",
        "- Do not copy exemplars verbatim; theyâ€™re hints only.\n",
        "\"\"\"\n",
        "\n",
        "USR = \"\"\"CLOCK: {clock}\n",
        "RESET: {reset}\n",
        "SIGNALS: {signals}\n",
        "REQUIREMENT: {requirement}\n",
        "\n",
        "RETRIEVED EXEMPLARS :\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", SYS), (\"user\", USR)])\n",
        "chain = prompt | llm\n",
        "\n",
        "def generate_sva(requirement: str,\n",
        "                 signals: dict,\n",
        "                 clock: str = \"clk\",\n",
        "                 reset: str = \"rst_n\",\n",
        "                 k: int = 30):\n",
        "    query = requirement + \" \" + \" \".join(signals.keys())\n",
        "    hits = retriever.invoke(query)\n",
        "    context = \"\\n\\n---\\n\".join([h.page_content for h in hits])\n",
        "    signallist = \", \".join([f\"{k}={v}\" for k, v in signals.items()])\n",
        "\n",
        "    res = chain.invoke({\n",
        "        \"clock\": clock,\n",
        "        \"reset\": reset,\n",
        "        \"signals\": signallist,\n",
        "        \"requirement\": requirement,\n",
        "        \"context\": context\n",
        "    })\n",
        "    out = pyjson.loads(res.content)\n",
        "    assert \"assertion\" in out and \"anti_vacuity_cover\" in out and \"notes\" in out\n",
        "    assert \"assert property\" in out[\"assertion\"]\n",
        "    assert \"cover property\" in out[\"anti_vacuity_cover\"]\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "9HPaM0a6tTYY",
      "metadata": {
        "id": "9HPaM0a6tTYY"
      },
      "outputs": [],
      "source": [
        "def display_sva(out: dict):\n",
        "    \"\"\"Pretty-print the SVA generator output dict.\"\"\"\n",
        "    a = (out.get(\"assertion\") or \"\").strip()\n",
        "    c = (out.get(\"anti_vacuity_cover\") or \"\").strip()\n",
        "    n = (out.get(\"notes\") or \"\").strip()\n",
        "    line = \"-\" * 60\n",
        "\n",
        "    print(f\"{line}\\nASSERTION\\n{line}\\n{a}\\n\")\n",
        "    print(f\"{line}\\nANTI-VACUITY COVER\\n{line}\\n{c}\\n\")\n",
        "    if n:\n",
        "        print(f\"{line}\\nNOTES\\n{line}\\n{n}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "4rjGekZ4qsuN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rjGekZ4qsuN",
        "outputId": "414cface-0e67-4ecc-867b-94b48518618c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------\n",
            "ASSERTION\n",
            "------------------------------------------------------------\n",
            "property data_stability; @(posedge clk) disable iff (!rst_n) (valid && !ready) |-> $stable(data); assert property (data_stability);\n",
            "\n",
            "------------------------------------------------------------\n",
            "ANTI-VACUITY COVER\n",
            "------------------------------------------------------------\n",
            "cover property (valid && !ready |-> (##[1:$] ready));\n",
            "\n",
            "------------------------------------------------------------\n",
            "NOTES\n",
            "------------------------------------------------------------\n",
            "This assertion ensures that when valid is high and ready is low, the data remains stable until ready goes high.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "resp = generate_sva(\n",
        "    requirement=\"When valid is high and ready is low, data must remain stable until ready goes high.\",\n",
        "    signals={\"valid\":\"valid\",\"ready\":\"ready\",\"data\":\"data[7:0]\"},\n",
        "    clock=\"clk\", reset=\"rst_n\"\n",
        ")\n",
        "display_sva(resp)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
