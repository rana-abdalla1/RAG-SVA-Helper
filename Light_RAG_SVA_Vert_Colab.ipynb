{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ipk_WED8mnRL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipk_WED8mnRL",
        "outputId": "7f255868-69c8-4278-bd25-7e3a9d32cf61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain==0.3.0 langchain-community==0.3.0 langchain-openai==0.2.0 \\\n",
        "                chromadb openai tiktoken\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"  # <-- paste your key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LZRo49kGmu_G",
      "metadata": {
        "id": "LZRo49kGmu_G"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "VERT_JSONL_PATH = \"/content/drive/MyDrive/VERT.json\"\n",
        "\n",
        "def load_vert_rows(path):\n",
        "    rows = []\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    rows.append(json.loads(line))\n",
        "                except:\n",
        "                    pass\n",
        "    return rows\n",
        "\n",
        "rows = load_vert_rows(VERT_JSONL_PATH)\n",
        "\n",
        "def rows_to_docs(rows, max_chars=1200):\n",
        "    docs = []\n",
        "    for r in rows:\n",
        "      # split into code and assertion sections to ensure consistent formatting\n",
        "        code = (r.get(\"Code\") or \"\").strip()\n",
        "        assertion = (r.get(\"Assertion\") or \"\").strip()\n",
        "\n",
        "        # Join both sections\n",
        "        text_parts = []\n",
        "        if code:\n",
        "            text_parts.append(f\"CODE:\\n{code[:max_chars]}\")\n",
        "        if assertion:\n",
        "            text_parts.append(f\"\\nASSERTION:\\n{assertion[:max_chars]}\")\n",
        "\n",
        "        text = \"\\n\".join(text_parts)\n",
        "        docs.append(Document(page_content=text, metadata={}))\n",
        "    return docs\n",
        "\n",
        "docs = rows_to_docs(rows)\n",
        "print(f\"Prepared {len(docs)} docs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "btYz_W21m4R-",
      "metadata": {
        "id": "btYz_W21m4R-"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "docs_split = splitter.split_documents(docs)\n",
        "\n",
        "vstore = Chroma(collection_name=\"vert_sva_gen\", embedding_function=emb)\n",
        "\n",
        "BATCH = 128\n",
        "for i in range(0, len(docs_split), BATCH):\n",
        "    vstore.add_documents(docs_split[i:i+BATCH])\n",
        "\n",
        "\n",
        "retriever = vstore.as_retriever(search_kwargs={\"k\": 10})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HA1mb3pjaZI-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA1mb3pjaZI-",
        "outputId": "8c90b27f-5b05-4129-df2b-a7a5a2b49bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 20000 total rows.\n",
            "Keeping 20000 rows for test embedding.\n",
            "Prepared 20000 docs for test run.\n",
            "\n",
            "--- Stored Item 1 ---\n",
            "CODE:\n",
            "if (  auth_6  && data_1 ) begin \n",
            "     sig_17 = data_10;\n",
            "     hw_13 = reg_9;\n",
            "    if ( data_15  && rx_6  && hw_6 ) begin\n",
            "        tx_2 = chip_8;\n",
            "        cfg_15 = clk_4;\n",
            "    end\n",
            "        if ( tx_16  && tx_14  && reg_12 ) begin\n",
            "            clk_13 = clk_3;\n",
            "            sig_116 = data_2;\n",
            "        end\n",
            "end\n",
            "\n",
            "ASSERTION:\n",
            "property ValidDataeotid; (  auth_6  && data_1 ) |=> sig_17 == data_10 && hw_13 == reg_9 ;endproperty \n",
            " \n",
            " property AUTHOKeotid; (  auth_6  && data_1 ) &&  (  data_15  && rx_6  && hw_6 ) |=> tx_2 == chip_8 && cfg_15 == clk_4 ;endproperty \n",
            " \n",
            " property DataSynceotid; (  auth_6  && data_1 ) &&  (  data_15  && rx_6  && hw_6 ) &&  (  tx_16  && tx_14  && reg_12 ) |=> clk_13 == clk_3 && sig_116 == data_2 ;endproperty ...\n",
            "\n",
            "Metadata keys: ['Code', 'Assertion', 'Synchronous']\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "VERT_JSONL_PATH = \"/content/drive/MyDrive/VERT.json\"\n",
        "\n",
        "\n",
        "def load_vert_rows(path):\n",
        "    rows = []\n",
        "    if os.path.exists(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    rows.append(json.loads(line))\n",
        "                except Exception:\n",
        "                    pass  \n",
        "    return rows\n",
        "\n",
        "rows = load_vert_rows(VERT_JSONL_PATH)\n",
        "print(f\"Loaded {len(rows)} total rows.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rows_to_docs(rows):\n",
        "    docs = []\n",
        "    for r in rows:\n",
        "        code = (r.get(\"Code\") or \"\").strip()\n",
        "        assertion = (r.get(\"Assertion\") or \"\").strip()\n",
        "\n",
        "        \n",
        "        content_parts = []\n",
        "        if code:\n",
        "            content_parts.append(\"CODE:\\n\" + code)\n",
        "        if assertion:\n",
        "            content_parts.append(\"ASSERTION:\\n\" + assertion)\n",
        "\n",
        "        page_content = \"\\n\\n\".join(content_parts) if content_parts else json.dumps(r, ensure_ascii=False)\n",
        "\n",
        "\n",
        "\n",
        "        docs.append(Document(page_content=page_content, metadata=dict(r)))\n",
        "    return docs\n",
        "\n",
        "docs = rows_to_docs(rows)\n",
        "\n",
        "\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vstore = Chroma(collection_name=\"vert_sva_gen_test\", embedding_function=emb)\n",
        "\n",
        "BATCH = 128\n",
        "for i in range(0, len(docs), BATCH):\n",
        "    vstore.add_documents(docs[i:i+BATCH])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UxwaHxnRm9h3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxwaHxnRm9h3",
        "outputId": "9245ea5f-3c29-46c7-b504-b29fb0a6882a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: WARNING! response_format is not default parameter.\n",
            "                response_format was transferred to model_kwargs.\n",
            "                Please confirm that response_format is what you intended.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import json as pyjson\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    response_format={\"type\": \"json_object\"},  \n",
        ")\n",
        "\n",
        "SYS = \"\"\"You generate strict SystemVerilog Assertions from a requirement and signals.\n",
        "Return only JSON with fields: assertion, anti_vacuity_cover, notes.\n",
        "\n",
        "HARD RULES:\n",
        "\n",
        "========================\n",
        "INPUTS (variables)\n",
        "========================\n",
        "Requirement (text): {requirement}\n",
        "Signals (JSON map): {signals_json}\n",
        "  # keys = logical names; values = HDL identifiers, e.g.\n",
        "  # {{\"valid\":\"vld_i\",\"ready\":\"rdy_i\",\"data\":\"payload[7:0]\"}}\n",
        "Clock: {clock}\n",
        "Reset (active as given): {reset}\n",
        "Optional params (JSON): {params_json}\n",
        "  # e.g. {{\"ACK_MAX_CYCLES\":3,\"LAT\":4}}\n",
        "\n",
        "========================\n",
        "RULES (style & correctness)\n",
        "========================\n",
        "- Use clear, descriptive, consistent names for sequences/properties (e.g., data_stable_until_ready).\n",
        "- If the antecedent is complex/reused, define it as a separate sequence and reuse it.\n",
        "- Always include explicit clock/reset on every property/cover:\n",
        "  @(posedge {clock}) disable iff (!{reset})\n",
        "- Add one short intent comment above each sequence/property.\n",
        "- Use $stable(x) and $past(x,N) for stability/history. NEVER write \"x == $stable(x)\".\n",
        "- Backpressure pattern (valid && !ready): use overlapped implication |-> so enforcement starts the same cycle.\n",
        "- Use until / until_with to hold until the release event; until_with includes the release cycle.\n",
        "- The cover must witness both the antecedent and the release/response event (e.g., (valid && !ready) ##[1:$] ready).\n",
        "- Do not copy RAG exemplars verbatim; adapt to the given requirement and signals.\n",
        "- Prefer parameters/localparams for time windows instead of magic numbers.\n",
        "- Output must include:\n",
        "  1) a named property + `assert property (NAME);`\n",
        "  2) one `cover property (...)` anti-vacuity witness (never assert a cover).\n",
        "- Synthesize-safe SVA only; no DPI or tool-specific pragmas.\n",
        "\n",
        "========================\n",
        "FEW-SHOT EXAMPLES (for pattern & format — adapt names/signals)\n",
        "========================\n",
        "-- Example 1: Backpressure — data must stay stable until READY\n",
        "// Stall begins when VALID is 1 and READY is 0; keep DATA stable during stall\n",
        "// and on the READY cycle (release included).\n",
        "sequence stall_s;\n",
        "  (valid && !ready);\n",
        "endsequence\n",
        "\n",
        "property data_stable_until_ready;\n",
        "  @(posedge clk) disable iff (!rst_n)\n",
        "    stall_s |-> $stable(data) until_with (ready);\n",
        "endproperty\n",
        "assert property (data_stable_until_ready);\n",
        "\n",
        "// Anti-vacuity: see a stall that eventually releases.\n",
        "cover property (@(posedge clk) disable iff (!rst_n)\n",
        "  stall_s ##[1:$] ready);\n",
        "\n",
        "// Notes:\n",
        "// - Overlapped |-> starts stability on the first stall cycle.\n",
        "// - until_with includes the release (READY) cycle.\n",
        "\n",
        "-- Example 2: Request must be acknowledged within N cycles\n",
        "localparam int ACK_MAX_CYCLES = 3; // set via params if provided\n",
        "\n",
        "// If REQ is high, ACK must arrive within 1..ACK_MAX_CYCLES cycles.\n",
        "property req_ack_within_n;\n",
        "  @(posedge clk) disable iff (!rst_n)\n",
        "    req |-> ##[1:ACK_MAX_CYCLES] ack;\n",
        "endproperty\n",
        "assert property (req_ack_within_n);\n",
        "\n",
        "// Cover: witness a bounded req→ack.\n",
        "cover property (@(posedge clk) disable iff (!rst_n)\n",
        "  req ##[1:ACK_MAX_CYCLES] ack);\n",
        "\n",
        "// Notes:\n",
        "// - Parameterized window avoids magic numbers.\n",
        "\n",
        "-- Example 3: Exact latency & value check with $past\n",
        "localparam int LAT = 4; // set via params if provided\n",
        "\n",
        "// If START, then exactly LAT cycles later DONE is 1 and RESULT equals the\n",
        "// input sampled at START.\n",
        "property result_latency_exact;\n",
        "  @(posedge clk) disable iff (!rst_n)\n",
        "    start |-> ##LAT (done && (result == $past(input_data, LAT)));\n",
        "endproperty\n",
        "assert property (result_latency_exact);\n",
        "\n",
        "// Cover: witness a START followed by DONE at LAT.\n",
        "cover property (@(posedge clk) disable iff (!rst_n)\n",
        "  start ##LAT done);\n",
        "\n",
        "// Notes:\n",
        "// - $past(input_data, LAT) binds RESULT to the sampled input at START.\n",
        "// - Exact-cycle latency check.\n",
        "\n",
        "========================\n",
        "GENERATION STEPS (apply to the current inputs)\n",
        "========================\n",
        "1) Parse {requirement}; map logical names to {signals_json}.\n",
        "2) Choose the closest pattern (hold-until, bounded response, exact latency, etc.).\n",
        "3) Name sequences/properties descriptively; parameterize any counts from {params_json} if present.\n",
        "4) Generate the three sections in the exact format.\n",
        "5) Quality gate (must all pass):\n",
        "   - Has @(posedge {clock}) disable iff (!{reset})\n",
        "   - Uses $stable/$past correctly (no \"x == $stable(x)\")\n",
        "   - Backpressure uses |-> if applicable\n",
        "   - No magic numbers (use params/localparams)\n",
        "   - Includes assert property (NAME);\n",
        "   - Includes a cover witnessing antecedent then release/response\n",
        "\"\"\"\n",
        "\n",
        "USR = \"\"\"CLOCK: {clock}\n",
        "RESET: {reset}\n",
        "SIGNALS (pretty): {signals}\n",
        "REQUIREMENT: {requirement}\n",
        "\n",
        "RETRIEVED EXEMPLARS:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", SYS), (\"user\", USR)])\n",
        "chain = prompt | llm\n",
        "\n",
        "def generate_sva(requirement: str,\n",
        "                 signals: dict,\n",
        "                 clock: str = \"clk\",\n",
        "                 reset: str = \"rst_n\",\n",
        "                 params: dict | None = None,\n",
        "                 k: int = 30):\n",
        "    query = requirement + \" \" + \" \".join(signals.keys())\n",
        "    hits = retriever.invoke(query)\n",
        "    context = \"\\n\\n---\\n\".join([h.page_content for h in hits])\n",
        "\n",
        "    signallist = \", \".join(f\"{k}={v}\" for k, v in signals.items())\n",
        "    signals_json = pyjson.dumps(signals, ensure_ascii=False)\n",
        "    params_json = pyjson.dumps(params or {}, ensure_ascii=False)\n",
        "\n",
        "    res = chain.invoke({\n",
        "        \"clock\": clock,\n",
        "        \"reset\": reset,\n",
        "        \"signals\": signallist,       # used by USR\n",
        "        \"signals_json\": signals_json, # used by SYS\n",
        "        \"params_json\": params_json,   # used by SYS\n",
        "        \"requirement\": requirement,\n",
        "        \"context\": context,\n",
        "    })\n",
        "\n",
        "    out = pyjson.loads(res.content)\n",
        "    assert \"assertion\" in out and \"anti_vacuity_cover\" in out and \"notes\" in out\n",
        "    assert \"assert property\" in out[\"assertion\"]\n",
        "    assert \"cover property\" in out[\"anti_vacuity_cover\"]\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "9HPaM0a6tTYY",
      "metadata": {
        "id": "9HPaM0a6tTYY"
      },
      "outputs": [],
      "source": [
        "def display_sva(out: dict):\n",
        "    \"\"\"Pretty-print the SVA generator output dict.\"\"\"\n",
        "    a = (out.get(\"assertion\") or \"\").strip()\n",
        "    c = (out.get(\"anti_vacuity_cover\") or \"\").strip()\n",
        "    n = (out.get(\"notes\") or \"\").strip()\n",
        "    line = \"-\" * 60\n",
        "\n",
        "    print(f\"{line}\\nASSERTION\\n{line}\\n{a}\\n\")\n",
        "    print(f\"{line}\\nANTI-VACUITY COVER\\n{line}\\n{c}\\n\")\n",
        "    if n:\n",
        "        print(f\"{line}\\nNOTES\\n{line}\\n{n}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "4rjGekZ4qsuN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rjGekZ4qsuN",
        "outputId": "c1f6967a-4e18-44a3-8436-b381958c3845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------\n",
            "ASSERTION\n",
            "------------------------------------------------------------\n",
            "property data_stable_until_ready; @(posedge clk) disable iff (!rst_n) (valid && !ready) |-> $stable(data) until_with (ready); endproperty assert property (data_stable_until_ready);\n",
            "\n",
            "------------------------------------------------------------\n",
            "ANTI-VACUITY COVER\n",
            "------------------------------------------------------------\n",
            "cover property (@(posedge clk) disable iff (!rst_n) (valid && !ready) ##[1:$] ready);\n",
            "\n",
            "------------------------------------------------------------\n",
            "NOTES\n",
            "------------------------------------------------------------\n",
            "- Overlapped |-> starts stability on the first stall cycle.\n",
            "- until_with includes the release (READY) cycle.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "resp = generate_sva(\n",
        "    requirement=\"When valid is high and ready is low, data must remain stable until ready goes high.\",\n",
        "    signals={\"valid\":\"valid\",\"ready\":\"ready\",\"data\":\"data[7:0]\"},\n",
        "    clock=\"clk\", reset=\"rst_n\"\n",
        ")\n",
        "display_sva(resp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
